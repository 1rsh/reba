{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inference/input_directory/yehvideo.mp4'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "FILEPATH = \"/Users/irsh/Downloads/construction.mp4\"\n",
    "\n",
    "shutil.copyfile(FILEPATH, 'inference/input_directory/yehvideo.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDNN_BENCHMARK: False\n",
      "DATALOADER:\n",
      "  ASPECT_RATIO_GROUPING: True\n",
      "  FILTER_EMPTY_ANNOTATIONS: True\n",
      "  NUM_WORKERS: 4\n",
      "  REPEAT_SQRT: True\n",
      "  REPEAT_THRESHOLD: 0.0\n",
      "  SAMPLER_TRAIN: TrainingSampler\n",
      "DATASETS:\n",
      "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
      "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
      "  PROPOSAL_FILES_TEST: ()\n",
      "  PROPOSAL_FILES_TRAIN: ()\n",
      "  TEST: ('keypoints_coco_2017_val',)\n",
      "  TRAIN: ('keypoints_coco_2017_train',)\n",
      "FLOAT32_PRECISION: \n",
      "GLOBAL:\n",
      "  HACK: 1.0\n",
      "INPUT:\n",
      "  CROP:\n",
      "    ENABLED: False\n",
      "    SIZE: [0.9, 0.9]\n",
      "    TYPE: relative_range\n",
      "  FORMAT: BGR\n",
      "  MASK_FORMAT: polygon\n",
      "  MAX_SIZE_TEST: 1333\n",
      "  MAX_SIZE_TRAIN: 1333\n",
      "  MIN_SIZE_TEST: 800\n",
      "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
      "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
      "  RANDOM_FLIP: horizontal\n",
      "MODEL:\n",
      "  ANCHOR_GENERATOR:\n",
      "    ANGLES: [[-90, 0, 90]]\n",
      "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
      "    NAME: DefaultAnchorGenerator\n",
      "    OFFSET: 0.0\n",
      "    SIZES: [[32], [64], [128], [256], [512]]\n",
      "  BACKBONE:\n",
      "    FREEZE_AT: 2\n",
      "    NAME: build_resnet_fpn_backbone\n",
      "  DEVICE: cpu\n",
      "  FPN:\n",
      "    FUSE_TYPE: sum\n",
      "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
      "    NORM: \n",
      "    OUT_CHANNELS: 256\n",
      "  KEYPOINT_ON: True\n",
      "  LOAD_PROPOSALS: False\n",
      "  MASK_ON: False\n",
      "  META_ARCHITECTURE: GeneralizedRCNN\n",
      "  PANOPTIC_FPN:\n",
      "    COMBINE:\n",
      "      ENABLED: True\n",
      "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
      "      OVERLAP_THRESH: 0.5\n",
      "      STUFF_AREA_LIMIT: 4096\n",
      "    INSTANCE_LOSS_WEIGHT: 1.0\n",
      "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
      "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
      "  PROPOSAL_GENERATOR:\n",
      "    MIN_SIZE: 0\n",
      "    NAME: RPN\n",
      "  RESNETS:\n",
      "    DEFORM_MODULATED: False\n",
      "    DEFORM_NUM_GROUPS: 1\n",
      "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
      "    DEPTH: 101\n",
      "    NORM: FrozenBN\n",
      "    NUM_GROUPS: 1\n",
      "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
      "    RES2_OUT_CHANNELS: 256\n",
      "    RES5_DILATION: 1\n",
      "    STEM_OUT_CHANNELS: 64\n",
      "    STRIDE_IN_1X1: True\n",
      "    WIDTH_PER_GROUP: 64\n",
      "  RETINANET:\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
      "    FOCAL_LOSS_ALPHA: 0.25\n",
      "    FOCAL_LOSS_GAMMA: 2.0\n",
      "    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']\n",
      "    IOU_LABELS: [0, -1, 1]\n",
      "    IOU_THRESHOLDS: [0.4, 0.5]\n",
      "    NMS_THRESH_TEST: 0.5\n",
      "    NORM: \n",
      "    NUM_CLASSES: 80\n",
      "    NUM_CONVS: 4\n",
      "    PRIOR_PROB: 0.01\n",
      "    SCORE_THRESH_TEST: 0.05\n",
      "    SMOOTH_L1_LOSS_BETA: 0.1\n",
      "    TOPK_CANDIDATES_TEST: 1000\n",
      "  ROI_BOX_CASCADE_HEAD:\n",
      "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
      "    IOUS: (0.5, 0.6, 0.7)\n",
      "  ROI_BOX_HEAD:\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
      "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
      "    CLS_AGNOSTIC_BBOX_REG: False\n",
      "    CONV_DIM: 256\n",
      "    FC_DIM: 1024\n",
      "    FED_LOSS_FREQ_WEIGHT_POWER: 0.5\n",
      "    FED_LOSS_NUM_CLASSES: 50\n",
      "    NAME: FastRCNNConvFCHead\n",
      "    NORM: \n",
      "    NUM_CONV: 0\n",
      "    NUM_FC: 2\n",
      "    POOLER_RESOLUTION: 7\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "    SMOOTH_L1_BETA: 0.5\n",
      "    TRAIN_ON_PRED_BOXES: False\n",
      "    USE_FED_LOSS: False\n",
      "    USE_SIGMOID_CE: False\n",
      "  ROI_HEADS:\n",
      "    BATCH_SIZE_PER_IMAGE: 512\n",
      "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
      "    IOU_LABELS: [0, 1]\n",
      "    IOU_THRESHOLDS: [0.5]\n",
      "    NAME: StandardROIHeads\n",
      "    NMS_THRESH_TEST: 0.5\n",
      "    NUM_CLASSES: 1\n",
      "    POSITIVE_FRACTION: 0.25\n",
      "    PROPOSAL_APPEND_GT: True\n",
      "    SCORE_THRESH_TEST: 0.7\n",
      "  ROI_KEYPOINT_HEAD:\n",
      "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
      "    NAME: KRCNNConvDeconvUpsampleHead\n",
      "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
      "    NUM_KEYPOINTS: 17\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "  ROI_MASK_HEAD:\n",
      "    CLS_AGNOSTIC_MASK: False\n",
      "    CONV_DIM: 256\n",
      "    NAME: MaskRCNNConvUpsampleHead\n",
      "    NORM: \n",
      "    NUM_CONV: 4\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "  RPN:\n",
      "    BATCH_SIZE_PER_IMAGE: 256\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
      "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
      "    BOUNDARY_THRESH: -1\n",
      "    CONV_DIMS: [-1]\n",
      "    HEAD_NAME: StandardRPNHead\n",
      "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']\n",
      "    IOU_LABELS: [0, -1, 1]\n",
      "    IOU_THRESHOLDS: [0.3, 0.7]\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    NMS_THRESH: 0.7\n",
      "    POSITIVE_FRACTION: 0.5\n",
      "    POST_NMS_TOPK_TEST: 1000\n",
      "    POST_NMS_TOPK_TRAIN: 1500\n",
      "    PRE_NMS_TOPK_TEST: 1000\n",
      "    PRE_NMS_TOPK_TRAIN: 2000\n",
      "    SMOOTH_L1_BETA: 0.0\n",
      "  SEM_SEG_HEAD:\n",
      "    COMMON_STRIDE: 4\n",
      "    CONVS_DIM: 128\n",
      "    IGNORE_VALUE: 255\n",
      "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    NAME: SemSegFPNHead\n",
      "    NORM: GN\n",
      "    NUM_CLASSES: 54\n",
      "  WEIGHTS: https://dl.fbaipublicfiles.com/detectron2/COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x/138363331/model_final_997cc7.pkl\n",
      "OUTPUT_DIR: ./output\n",
      "SEED: -1\n",
      "SOLVER:\n",
      "  AMP:\n",
      "    ENABLED: False\n",
      "  BASE_LR: 0.02\n",
      "  BASE_LR_END: 0.0\n",
      "  BIAS_LR_FACTOR: 1.0\n",
      "  CHECKPOINT_PERIOD: 5000\n",
      "  CLIP_GRADIENTS:\n",
      "    CLIP_TYPE: value\n",
      "    CLIP_VALUE: 1.0\n",
      "    ENABLED: False\n",
      "    NORM_TYPE: 2.0\n",
      "  GAMMA: 0.1\n",
      "  IMS_PER_BATCH: 16\n",
      "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
      "  MAX_ITER: 270000\n",
      "  MOMENTUM: 0.9\n",
      "  NESTEROV: False\n",
      "  NUM_DECAYS: 3\n",
      "  REFERENCE_WORLD_SIZE: 0\n",
      "  RESCALE_INTERVAL: False\n",
      "  STEPS: (210000, 250000)\n",
      "  WARMUP_FACTOR: 0.001\n",
      "  WARMUP_ITERS: 1000\n",
      "  WARMUP_METHOD: linear\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "  WEIGHT_DECAY_BIAS: None\n",
      "  WEIGHT_DECAY_NORM: 0.0\n",
      "TEST:\n",
      "  AUG:\n",
      "    ENABLED: False\n",
      "    FLIP: True\n",
      "    MAX_SIZE: 4000\n",
      "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
      "  DETECTIONS_PER_IMAGE: 100\n",
      "  EVAL_PERIOD: 0\n",
      "  EXPECTED_RESULTS: []\n",
      "  KEYPOINT_OKS_SIGMAS: []\n",
      "  PRECISE_BN:\n",
      "    ENABLED: False\n",
      "    NUM_ITER: 200\n",
      "VERSION: 2\n",
      "VIS_PERIOD: 0\n",
      "\u001b[32m[11/05 14:22:34 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x/138363331/model_final_997cc7.pkl ...\n",
      "Processing input_directory/yehvideo.mp4\n",
      "ffmpeg version 7.0.2 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with Apple clang version 15.0.0 (clang-1500.3.9.4)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.0.2 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59.  8.100 / 59.  8.100\n",
      "  libavcodec     61.  3.100 / 61.  3.100\n",
      "  libavformat    61.  1.100 / 61.  1.100\n",
      "  libavdevice    61.  1.100 / 61.  1.100\n",
      "  libavfilter    10.  1.100 / 10.  1.100\n",
      "  libswscale      8.  1.100 /  8.  1.100\n",
      "  libswresample   5.  1.100 /  5.  1.100\n",
      "  libpostproc    58.  1.100 / 58.  1.100\n",
      "2024-11-05 14:22:34.145 ffmpeg[18496:3814476] WARNING: Add NSCameraUseContinuityCameraDeviceType to your Info.plist to use AVCaptureDeviceTypeContinuityCamera.\n",
      "\u001b[0;35m[avfoundation @ 0x129f051c0] \u001b[0m\u001b[1;31mSelected pixel format (yuv420p) is not supported by the input device.\n",
      "\u001b[0m\u001b[0;35m[avfoundation @ 0x129f051c0] \u001b[0m\u001b[1;31mSupported pixel formats:\n",
      "\u001b[0m\u001b[0;35m[avfoundation @ 0x129f051c0] \u001b[0m\u001b[1;31m  uyvy422\n",
      "\u001b[0m\u001b[0;35m[avfoundation @ 0x129f051c0] \u001b[0m\u001b[1;31m  yuyv422\n",
      "\u001b[0m\u001b[0;35m[avfoundation @ 0x129f051c0] \u001b[0m\u001b[1;31m  nv12\n",
      "\u001b[0m\u001b[0;35m[avfoundation @ 0x129f051c0] \u001b[0m\u001b[1;31m  0rgb\n",
      "\u001b[0m\u001b[0;35m[avfoundation @ 0x129f051c0] \u001b[0m\u001b[1;31m  bgr0\n",
      "\u001b[0m\u001b[0;35m[avfoundation @ 0x129f051c0] \u001b[0m\u001b[0;33mOverriding selected pixel format to use uyvy422 instead.\n",
      "\u001b[0mInput #0, avfoundation, from '0':\n",
      "  Duration: N/A, start: 348070.436920, bitrate: N/A\n",
      "  Stream #0:0: Video: rawvideo (UYVY / 0x59565955), uyvy422, 640x480, 23.08 tbr, 1000k tbn\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (rawvideo (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0musing cpu capabilities: ARMv8 NEON\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mprofile High 4:2:2, level 3.0, 4:2:2, 8-bit\n",
      "Output #0, tee, to '[f=rawvideo]pipe:1|input_directory/yehvideo.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf61.1.100\n",
      "  Stream #0:0: Video: h264, yuv422p(progressive), 640x480, q=2-31, 23.08 fps, 23.08 tbn\n",
      "      Metadata:\n",
      "        encoder         : Lavc61.3.100 libx264\n",
      "      Side data:\n",
      "        cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "Traceback (most recent call last):A time=00:00:03.24 bitrate=N/A speed=0.587x     \n",
      "  File \"/Users/irsh/Documents/workdir/Improved-reba-Deep-learning-based-rapid-entire-body-assessment/inference/infer_video_d2.py\", line 189, in <module>\n",
      "    main(args)\n",
      "  File \"/Users/irsh/Documents/workdir/Improved-reba-Deep-learning-based-rapid-entire-body-assessment/inference/infer_video_d2.py\", line 141, in main\n",
      "    for frame_i, im in enumerate(read_camera(video_name)):\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/irsh/Documents/workdir/Improved-reba-Deep-learning-based-rapid-entire-body-assessment/inference/infer_video_d2.py\", line 103, in read_camera\n",
      "    data = pipe.stdout.read(w * h * 3)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\u001b[1;35m[tee @ 0x308e176e0] \u001b[0m\u001b[1;31mSlave muxer #0 failed: Broken pipe, continuing with 1/2 slaves.\n",
      "\u001b[0m\u001b[1;35m[out#0/tee @ 0x12ad41230] \u001b[0mvideo:831KiB audio:0KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: unknown\n",
      "frame=  149 fps= 24 q=-1.0 Lsize=N/A time=00:00:06.36 bitrate=N/A speed=1.02x    \n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mframe I:1     Avg QP:24.77  size: 28229\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mframe P:37    Avg QP:24.46  size: 13339\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mframe B:111   Avg QP:27.99  size:  2965\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mconsecutive B-frames:  0.7%  0.0%  0.0% 99.3%\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mmb I  I16..4:  4.0% 69.7% 26.3%\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mmb P  I16..4:  2.0%  9.2%  2.0%  P16..4: 46.5% 25.1% 12.4%  0.0%  0.0%    skip: 2.7%\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mmb B  I16..4:  0.1%  0.3%  0.0%  B16..8: 47.0%  8.3%  1.3%  direct: 4.9%  skip:38.0%  L0:42.9% L1:44.9% BI:12.3%\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0m8x8 transform intra:70.1% inter:74.6%\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mcoded y,uvDC,uvAC intra: 66.9% 81.8% 27.8% inter: 24.3% 44.6% 1.1%\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mi16 v,h,dc,p: 25% 15%  6% 53%\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 23% 14% 21%  5%  6%  9%  6%  6%  9%\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 33% 15% 13%  5%  7% 10%  5%  5%  6%\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mi8c dc,h,v,p: 48% 11% 32% 10%\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mWeighted P-Frames: Y:18.9% UV:16.2%\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mref P L0: 49.5% 20.6% 19.0%  9.0%  1.8%\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mref B L0: 88.1%  8.8%  3.1%\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mref B L1: 96.4%  3.6%\n",
      "\u001b[1;36m[libx264 @ 0x308e18060] \u001b[0mkb/s:1054.49\n"
     ]
    }
   ],
   "source": [
    "!cd inference && python infer_video_d2.py --cfg COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml --output-dir output_directory --image-ext mp4 input_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 2D detections from ../inference/output_directory\n",
      "Processing ../inference/output_directory/videoplayback.obj\n",
      "60 total frames processed\n",
      "0 frames were interpolated\n",
      "----------\n",
      "Saving...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!cd data && python prepare_data_2d_custom.py -i ../inference/output_directory -o yehmyvideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset='custom', keypoints='yehmyvideo', subjects_train='S1,S5,S6,S7,S8', subjects_test='S9,S11', subjects_unlabeled='', actions='*', checkpoint='checkpoint', checkpoint_frequency=10, resume='', evaluate='reba_pose.bin', render=True, by_subject=False, export_training_curves=False, stride=1, epochs=60, batch_size=1024, dropout=0.25, learning_rate=0.001, lr_decay=0.95, data_augmentation=True, test_time_augmentation=True, architecture='3,3,3,3,3', causal=False, channels=1024, subset=1, downsample=1, warmup=1, no_eval=False, dense=False, disable_optimizations=False, linear_projection=False, bone_length_term=True, no_proj=False, viz_subject='videoplayback', viz_action='custom', viz_camera=0, viz_video='inference/input_directory/yehvideo.mp4', viz_skip=0, viz_output='new_op.mp4', viz_export='outputfile', viz_bitrate=3000, viz_no_ground_truth=False, viz_limit=-1, viz_downsample=1, viz_size=6)\n",
      "Loading dataset...\n",
      "Preparing data...\n",
      "Loading 2D detections...\n",
      "INFO: Receptive field: 243 frames\n",
      "INFO: Trainable parameter count: 16952371\n",
      "Loading checkpoint checkpoint/reba_pose.bin\n",
      "/Users/irsh/Documents/workdir/Improved-reba-Deep-learning-based-rapid-entire-body-assessment/run.py:206: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(chk_filename, map_location=lambda storage, loc: storage)\n",
      "This model was trained for 80 epochs\n",
      "INFO: Testing on 60 frames\n",
      "Rendering...\n",
      "INFO: this action is unlabeled. Ground truth will not be rendered.\n",
      "Exporting joint positions to outputfile\n",
      "ffmpeg version 7.0.2 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with Apple clang version 15.0.0 (clang-1500.3.9.4)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.0.2 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59.  8.100 / 59.  8.100\n",
      "  libavcodec     61.  3.100 / 61.  3.100\n",
      "  libavformat    61.  1.100 / 61.  1.100\n",
      "  libavdevice    61.  1.100 / 61.  1.100\n",
      "  libavfilter    10.  1.100 / 10.  1.100\n",
      "  libswscale      8.  1.100 /  8.  1.100\n",
      "  libswresample   5.  1.100 /  5.  1.100\n",
      "  libpostproc    58.  1.100 / 58.  1.100\n",
      "\u001b[0;33m-vsync is deprecated. Use -fps_mode\n",
      "\u001b[0m\u001b[0;33mPassing a number to -vsync is deprecated, use a string argument as described in the manual.\n",
      "\u001b[0mInput #0, mov,mp4,m4a,3gp,3g2,mj2, from 'inference/input_directory/yehvideo.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf61.1.100\n",
      "  Duration: 00:00:06.45, start: 0.000000, bitrate: 1057 kb/s\n",
      "  Stream #0:0[0x1](und): Video: h264 (High 4:2:2) (avc1 / 0x31637661), yuv422p(progressive), 640x480, 1054 kb/s, 23.08 fps, 23.08 tbr, 17728 tbn (default)\n",
      "      Metadata:\n",
      "        handler_name    : VideoHandler\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : Lavc61.3.100 libx264\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (h264 (native) -> rawvideo (native))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;34m[swscaler @ 0x178238000] \u001b[0m\u001b[1;34m[swscaler @ 0x1281c8000] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv422p to rgb24.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x178238000] \u001b[0m\u001b[1;34m[swscaler @ 0x1281d8000] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv422p to rgb24.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x178238000] \u001b[0m\u001b[1;34m[swscaler @ 0x1281e8000] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv422p to rgb24.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x178238000] \u001b[0m\u001b[1;34m[swscaler @ 0x1281f8000] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv422p to rgb24.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x178238000] \u001b[0m\u001b[1;34m[swscaler @ 0x128208000] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv422p to rgb24.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x178238000] \u001b[0m\u001b[1;34m[swscaler @ 0x128218000] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv422p to rgb24.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x178238000] \u001b[0m\u001b[1;34m[swscaler @ 0x128228000] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv422p to rgb24.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x178238000] \u001b[0m\u001b[1;34m[swscaler @ 0x128238000] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv422p to rgb24.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x178238000] \u001b[0m\u001b[1;34m[swscaler @ 0x308158000] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv422p to rgb24.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x178238000] \u001b[0m\u001b[1;34m[swscaler @ 0x308168000] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv422p to rgb24.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x178238000] \u001b[0m\u001b[1;34m[swscaler @ 0x308178000] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv422p to rgb24.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x178238000] \u001b[0m\u001b[1;34m[swscaler @ 0x308188000] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv422p to rgb24.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x178238000] \u001b[0m\u001b[1;34m[swscaler @ 0x308198000] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv422p to rgb24.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x178238000] \u001b[0m\u001b[1;34m[swscaler @ 0x3081a8000] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv422p to rgb24.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x178238000] \u001b[0m\u001b[1;34m[swscaler @ 0x3081b8000] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv422p to rgb24.\n",
      "\u001b[0mOutput #0, image2pipe, to 'pipe:':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf61.1.100\n",
      "  Stream #0:0(und): Video: rawvideo (RGB[24] / 0x18424752), rgb24(pc, gbr/unknown/unknown, progressive), 640x480, q=2-31, 170188 kb/s, 23.08 fps, 23.08 tbn (default)\n",
      "      Metadata:\n",
      "        handler_name    : VideoHandler\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : Lavc61.3.100 rawvideo\n",
      "\u001b[1;35m[out#0/image2pipe @ 0x1547076e0] \u001b[0mvideo:134100KiB audio:0KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.000000%\n",
      "frame=  149 fps=0.0 q=-0.0 Lsize=  134100KiB time=00:00:06.45 bitrate=170188.8kbits/s speed= 115x    \n",
      "/Users/irsh/Documents/workdir/Improved-reba-Deep-learning-based-rapid-entire-body-assessment/common/visualization.py:202: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  fig.tight_layout()\n",
      "59/60      \r"
     ]
    }
   ],
   "source": [
    "!python run.py -d custom -k yehmyvideo -arc 3,3,3,3,3 -c checkpoint --evaluate reba_pose.bin --render --viz-subject videoplayback --viz-action custom --viz-camera 0 --viz-video inference/input_directory/yehvideo.mp4 --viz-output new_op.mp4 --viz-export outputfile --viz-size 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.shape= (60, 17, 3)\n",
      "OpenCV: Couldn't read video stream from file \"output.mp4\"\n",
      "Frame：0\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 2\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 12,4\n",
      "Frame：1\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：2\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：3\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 14,4\n",
      "Frame：4\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 14,4\n",
      "Frame：5\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：6\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 14,4\n",
      "Frame：7\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：8\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：9\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：10\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 14,4\n",
      "Frame：11\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 14,4\n",
      "Frame：12\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 2\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 13,4\n",
      "Frame：13\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 2\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 13,4\n",
      "Frame：14\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 14,4\n",
      "Frame：15\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 14,4\n",
      "Frame：16\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：17\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 2\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 13,4\n",
      "Frame：18\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 4\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 14,4\n",
      "Frame：19\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：20\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 4\n",
      "upper_arms_score= 1\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 13,4\n",
      "Frame：21\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 4\n",
      "upper_arms_score= 2\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：22\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 4\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 14,4\n",
      "Frame：23\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：24\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 4\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 14,4\n",
      "Frame：25\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 4\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 14,4\n",
      "Frame：26\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 4\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 14,4\n",
      "Frame：27\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 4\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 14,4\n",
      "Frame：28\n",
      "trunk_score= 4\n",
      "neck_score= 2\n",
      "legs_score= 4\n",
      "upper_arms_score= 2\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 14,4\n",
      "Frame：29\n",
      "trunk_score= 3\n",
      "neck_score= 2\n",
      "legs_score= 4\n",
      "upper_arms_score= 1\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 12,4\n",
      "Frame：30\n",
      "trunk_score= 3\n",
      "neck_score= 2\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：31\n",
      "trunk_score= 3\n",
      "neck_score= 2\n",
      "legs_score= 4\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 14,4\n",
      "Frame：32\n",
      "trunk_score= 3\n",
      "neck_score= 2\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 14,4\n",
      "Frame：33\n",
      "trunk_score= 3\n",
      "neck_score= 2\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：34\n",
      "trunk_score= 3\n",
      "neck_score= 2\n",
      "legs_score= 4\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 14,4\n",
      "Frame：35\n",
      "trunk_score= 3\n",
      "neck_score= 2\n",
      "legs_score= 3\n",
      "upper_arms_score= 2\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 12,4\n",
      "Frame：36\n",
      "trunk_score= 3\n",
      "neck_score= 2\n",
      "legs_score= 4\n",
      "upper_arms_score= 2\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：37\n",
      "trunk_score= 3\n",
      "neck_score= 2\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：38\n",
      "trunk_score= 3\n",
      "neck_score= 2\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：39\n",
      "trunk_score= 4\n",
      "neck_score= 2\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 14,4\n",
      "Frame：40\n",
      "trunk_score= 4\n",
      "neck_score= 2\n",
      "legs_score= 2\n",
      "upper_arms_score= 2\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 13,4\n",
      "Frame：41\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 2\n",
      "upper_arms_score= 4\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：42\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 4\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 14,4\n",
      "Frame：43\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 4\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 14,4\n",
      "Frame：44\n",
      "trunk_score= 4\n",
      "neck_score= 2\n",
      "legs_score= 4\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 15,4\n",
      "Frame：45\n",
      "trunk_score= 4\n",
      "neck_score= 2\n",
      "legs_score= 4\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 16,4\n",
      "Frame：46\n",
      "trunk_score= 4\n",
      "neck_score= 2\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 15,4\n",
      "Frame：47\n",
      "trunk_score= 4\n",
      "neck_score= 2\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 15,4\n",
      "Frame：48\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：49\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：50\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：51\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：52\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：53\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：54\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：55\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：56\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 14,4\n",
      "Frame：57\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 2\n",
      "reba_score and action_level = 14,4\n",
      "Frame：58\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "Frame：59\n",
      "trunk_score= 4\n",
      "neck_score= 1\n",
      "legs_score= 3\n",
      "upper_arms_score= 3\n",
      "lower_arms_score= 1\n",
      "reba_score and action_level = 13,4\n",
      "action_levels= [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
